{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f734ca28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../../\")) #vscode import \n",
    "\n",
    "from settings.settings import USDA_NASS_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2212f423",
   "metadata": {},
   "source": [
    "Export sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03610934",
   "metadata": {},
   "outputs": [],
   "source": [
    "html = requests.get('https://apps.fas.usda.gov/export-sales/h107.htm').text\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "table = soup.find('table')\n",
    "\n",
    "rows = []\n",
    "for tr in table.find_all(\"tr\"):\n",
    "    cells = [td.get_text(strip=True) for td in tr.find_all(\"td\")]\n",
    "    rows.append(cells)\n",
    "\n",
    "cols = []\n",
    "for i, j in zip(rows[1], rows[2]):\n",
    "    if f\"{i} {j}\" not in cols:\n",
    "        cols.append(f\"{i} {j}\")\n",
    "    else:\n",
    "        cols.append(f\"NMY {i} {j}\")\n",
    "\n",
    "data = rows[4:]\n",
    "df = pd.DataFrame(data, columns=cols)\n",
    "df = df.dropna()\n",
    "df.to_csv('../../data/US_export_sales/export_sales.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0358c30b",
   "metadata": {},
   "source": [
    "Crop progress / condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebf0570",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_data(category):\n",
    "    endpoint = f\"https://quickstats.nass.usda.gov/api/api_GET/?key={USDA_NASS_API_KEY}\"\n",
    "    request_params = f\"source_desc=SURVEY&sector_desc=CROPS&group_desc=FIELD CROPS&commodity_desc=WHEAT&statisticcat_desc={category}\" \\\n",
    "                    \"&agg_level_desc=NATIONAL&class_desc=WINTER\" \\\n",
    "                    \"&format=JSON\"\n",
    "\n",
    "    url = '&'.join([endpoint, request_params])\n",
    "\n",
    "    r = requests.get(url)\n",
    "    df = pd.DataFrame(r.json()['data'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d0d467",
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = download_data('CONDITION', 'WINTER')\n",
    "progress = download_data('PROGRESS', 'WINTER')\n",
    "\n",
    "conditions.to_csv('../../data/crop_progress/conditions.csv', index=False)\n",
    "progress.to_csv('../../data/crop_progress/progress.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c91cc7",
   "metadata": {},
   "source": [
    "Grain Stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "d4f6bd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_grain_stocks_txt(data):\n",
    "    #Format and split lines\n",
    "    data = data.replace('\\r', '')\n",
    "    splits = data.split('\\n')\n",
    "    lines = [line.strip() for line in splits if line]\n",
    "\n",
    "    #Find table of contents, titles that interest us, then keep only the data table\n",
    "    try:\n",
    "        content_start = lines.index('Contents')\n",
    "        title_domestic = lines[content_start + 1].split('.')[0]\n",
    "        title_mt = lines[content_start + 2].split('.')[0]\n",
    "    except:\n",
    "        print('No contents table')\n",
    "        return pd.DataFrame({})\n",
    "\n",
    "    content_formatted = [line.split('.')[0] for line in lines]\n",
    "    content_end = content_formatted.index('Information Contacts')\n",
    "\n",
    "    tables = lines[content_end:]\n",
    "    title_domestic_idx = tables.index(title_domestic)\n",
    "    title_mt_idx = tables.index(title_mt)\n",
    "\n",
    "    stocks = tables[title_domestic_idx:title_mt_idx]\n",
    "    table_start = next((i for i, item in enumerate(stocks) if item.startswith('---')), None)\n",
    "    stocks = stocks[table_start:title_mt_idx]\n",
    "\n",
    "    #Start getting data from data table, here years, headers and separate the data\n",
    "    year_line = stocks[1]\n",
    "    headers = stocks[3]\n",
    "    stocks_data = stocks[8:-3]\n",
    "\n",
    "    year_list = year_line.replace(' ', '').split(':')[1:]\n",
    "    cols = list(headers.replace(' ', '').split(':'))\n",
    "    cols.insert(0, 'Commodity')\n",
    "\n",
    "    #Find commodity name for each data then add the commodity name at the start of the string data\n",
    "    commodity = None\n",
    "    filtered_data = []\n",
    "    for data in stocks_data:\n",
    "        if len(data.split(':')[1]) == 0 and len(data.split(':')[0]) >= 2:\n",
    "            commodity = data.split(':')[0].strip()\n",
    "        elif len(data.split(':')[1]) > 0:\n",
    "            filtered_data.append(f'{commodity} {data}')\n",
    "\n",
    "    #Make a list of formatted data with a list of data string\n",
    "    result = []\n",
    "    for line in filtered_data:\n",
    "        parts = line.split(':')\n",
    "        name_part = parts[0].strip().replace('.', '')\n",
    "        \n",
    "        #Separate product and date\n",
    "        match = re.match(r'(.+?)\\s+((?:March|June|September|December)\\s+\\d+.*)', name_part)\n",
    "        \n",
    "        if match:\n",
    "            product = match.group(1).strip()\n",
    "            date = match.group(2).strip()\n",
    "        else:\n",
    "            product = name_part\n",
    "            date = ''\n",
    "        \n",
    "        #Extract numbers\n",
    "        numbers = parts[1].strip().split()\n",
    "        numbers = [int(num.replace(',', '')) for num in numbers]\n",
    "\n",
    "        result.append([product, date] + numbers)\n",
    "\n",
    "    #Make a dataframe with a the formatted list\n",
    "    rows = []\n",
    "    for row in result:\n",
    "        commodity = row[0]\n",
    "        date = row[1]\n",
    "        if len(row) >= 5:\n",
    "            rows.append([commodity, date, row[2], row[3], row[4], year_list[0]])\n",
    "        if len(row) >= 8:\n",
    "            rows.append([commodity, date, row[5], row[6], row[7], year_list[1]])\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=['Commodity', 'Date', 'On', 'Off', 'Totalall', 'Year'])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "e4173e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = 'https://esmis.nal.usda.gov'\n",
    "\n",
    "report_dates = pd.date_range(datetime(2010, 6, 1), datetime(2026, 1, 1), freq='3ME')\n",
    "report_dates = report_dates.where(report_dates.month != 12, report_dates + pd.DateOffset(months=1))\n",
    "report_dates = sorted(report_dates.append(pd.DatetimeIndex(['2019-02-01']))) #We manually add this report date bc of a rescheduled report\n",
    "\n",
    "urls = {}\n",
    "for date in report_dates:\n",
    "    report_year_month = date.strftime(\"%Y-%m\")\n",
    "\n",
    "    html = requests.get(f'{endpoint}/publication/grain-stocks?date={report_year_month}').text\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    links = soup.find_all('a')\n",
    "    for link in links: \n",
    "        if link.time:\n",
    "            full_link_date = datetime.fromisoformat(link.time['datetime'].replace(\"Z\", \"+00:00\"))\n",
    "            link_date = full_link_date.strftime(\"%Y-%m-%d\")\n",
    "            link_year_month = full_link_date.strftime(\"%Y-%m\")\n",
    "            if link_year_month == report_year_month and link['href'].endswith('.txt'):\n",
    "                urls[link_date] = link['href']\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "c1afb966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No contents table\n"
     ]
    }
   ],
   "source": [
    "full_df = []\n",
    "for report_date, url in urls.items():\n",
    "    data = requests.get(f'{endpoint}{url}').text\n",
    "    df = format_grain_stocks_txt(data)\n",
    "    df['Report_date'] = report_date\n",
    "    full_df.append(df)\n",
    "grain_stocks = pd.concat(full_df).reset_index(drop=True)\n",
    "grain_stocks.to_csv('../../data/grain_stocks/grain_stocks.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff435ba2",
   "metadata": {},
   "source": [
    "Rivers Water Levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9beac314",
   "metadata": {},
   "outputs": [],
   "source": [
    "location_dict = {\n",
    "    \"Mississippi River at St. Louis, MO\": \"USGS-07010000\",\n",
    "    \"Ohio River at Cincinnati, OH\": \"USGS-03255000\",\n",
    "    \"Illinois River at Meredosia, IL\": \"USGS-05585500\",\n",
    "    \"Ohio River at Louisville, KY\": \"USGS-03294500\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3347b712",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_rivers(location_id):    \n",
    "    url =  \"https://api.waterdata.usgs.gov/ogcapi/v0/collections/daily/items?f=json&lang=en-US&limit=50000&skipGeometry=false&sortby=time&offset=0&\" \\\n",
    "          f\"monitoring_location_id={location_id}&parameter_code=00065\"\n",
    "    r = requests.get(url)\n",
    "    data = r.json()['features']\n",
    "    data_list = [d['properties'] for d in data]\n",
    "    return pd.DataFrame(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "621f50f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = []\n",
    "for k, v in location_dict.items():\n",
    "    df = download_rivers(v)\n",
    "    df['location'] = k\n",
    "    data_list.append(df)\n",
    "rivers = pd.concat(data_list)\n",
    "\n",
    "rivers.to_csv('../../data/rivers/rivers_gage_height.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59a223d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
