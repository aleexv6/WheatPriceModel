{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf75f2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore import UNSIGNED\n",
    "from botocore.config import Config\n",
    "from boto3.s3.transfer import TransferConfig\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "import concurrent.futures\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63b7dad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "places_coordinates = {\n",
    "    \"Europe\": {'lat_min': 35.25, 'lat_max': 71.75, 'lon_min': -10.25, 'lon_max': 31},\n",
    "    \"Canada\": {'lat_min': 44.0, 'lat_max': 70.0, 'lon_min': -141.5, 'lon_max': -51.5}, \n",
    "    \"US\": {'lat_min': 24, 'lat_max': 49, 'lon_min': -125.5, 'lon_max': -65.5},\n",
    "    \"BlackSea\": {'lat_min': 41.25, 'lat_max': 57.0, 'lon_min':22.75, 'lon_max': 61.5},\n",
    "    \"Australia\": {'lat_min': -39, 'lat_max': -10.5, 'lon_min': 112.75, 'lon_max': 153.25},\n",
    "    \"Argentina\": {'lat_min': -56.25, 'lat_max': -21.75, 'lon_min': -75.5, 'lon_max': -51.75}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aece7d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_dates(year, max_retries=5, start_date=False):\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            r = requests.get(f'https://tds.gdex.ucar.edu/thredds/catalog/files/g/d084001/{year}/catalog.html', timeout=10)\n",
    "            break\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            retries += 1\n",
    "            time.sleep(5)\n",
    "            \n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "    dates = [f.get_text() for f in soup.find_all('code') if re.fullmatch(r\"\\d{8}\", f.get_text())]\n",
    "    if start_date:\n",
    "        return [d for d in dates if datetime.strptime(d, '%Y%m%d') >= datetime.strptime(str(start_date), '%Y%m%d')]\n",
    "    else:\n",
    "        return dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b9a380e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_files(year, date, run, forecast_horizon, max_retries=5):\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            r = requests.get(f'https://tds.gdex.ucar.edu/thredds/catalog/files/g/d084001/{year}/{date}/catalog.html')\n",
    "            break\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            retries += 1\n",
    "            time.sleep(5)\n",
    "            \n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "    files =  [f.get_text() for f in soup.find_all('code') if f.get_text()[-6:] == '.grib2']\n",
    "    current_run = [f for f in files if f.split('.')[2][-2:] == run  # check if run is 00\n",
    "                   and int(f.split('.f')[1][:3]) > 0 #remove the first data, where data is initial values of the model and not forecast\n",
    "                   and int(f.split('.f')[1][:3]) <= forecast_horizon] #where files are max forecast horizon\n",
    "    return current_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f1ae4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_single_file(year, date, file, coords, filepath, max_retries=5):\n",
    "    forecast_hour = int(file.split('.f')[1][:3])\n",
    "    forecast_hour_timedelta = timedelta(hours=forecast_hour)\n",
    "    ref_date =  datetime.strptime(file.split('.')[2], \"%Y%m%d%H\")\n",
    "    final_dt = ref_date + forecast_hour_timedelta\n",
    "    iso_date = final_dt.strftime(\"%Y-%m-%dT%H:%M:%SZ\")        \n",
    "\n",
    "    filename = f\"gfs.0p25.{iso_date.replace(':','-')}.nc\"\n",
    "\n",
    "    endpoint = \"https://tds.gdex.ucar.edu/thredds/ncss/grid/files/g/d084001/\"    \n",
    "    \n",
    "    var_precip = [\n",
    "        'Total_precipitation_surface_3_Hour_Accumulation',\n",
    "        'Total_precipitation_surface_6_Hour_Accumulation',\n",
    "        'Total_precipitation_surface_Mixed_intervals_Accumulation'\n",
    "    ]\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        for var in var_precip:\n",
    "            try:\n",
    "                url = (\n",
    "                    f\"{year}/{date}/{file}\"\n",
    "                    f\"?var=Temperature_height_above_ground\"\n",
    "                    f\"&var={var}\"\n",
    "                    f\"&north={coords['lat_max']}&west={coords['lon_min']}\"\n",
    "                    f\"&east={coords['lon_max']}&south={coords['lat_min']}\"\n",
    "                    f\"&horizStride=1\"\n",
    "                    f\"&time_start={iso_date}&time_end={iso_date}\"\n",
    "                    f\"&accept=netcdf3\"\n",
    "                )\n",
    "\n",
    "                r = requests.get(endpoint + url, timeout=10)\n",
    "                r.raise_for_status()\n",
    "                content_type = r.headers.get(\"Content-Type\", \"\").lower()\n",
    "                if \"netcdf\" in content_type and len(r.content) > 1000:\n",
    "                    with open(os.path.join(filepath, filename), \"wb\") as f:\n",
    "                        f.write(r.content)\n",
    "                    return True\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "            except requests.exceptions.RequestException:\n",
    "                continue\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "    print(\"Max retries reached. Download failed for\", file)\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b2e2dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_files(year, date, place):\n",
    "    time_vars = ['time1', 'time2']\n",
    "    height_vars = ['height_above_ground1', 'height_above_ground2', 'height_above_ground3']\n",
    "    precip_vars = ['Total_precipitation_surface_3_Hour_Accumulation', 'Total_precipitation_surface_6_Hour_Accumulation', 'Total_precipitation_surface_Mixed_intervals_Accumulation']\n",
    "    time_bounds_vars = ['time1_bounds', 'time2_bounds']\n",
    "\n",
    "    def fix_coords(ds):\n",
    "        if ds.longitude.max() > 180:\n",
    "            ds = ds.assign_coords(longitude=((ds.longitude + 180) % 360) - 180)\n",
    "            ds = ds.sortby(\"longitude\")\n",
    "\n",
    "        return ds\n",
    "    \n",
    "    def normalize_time(ds, var):\n",
    "        time_dim_candidates = [d for d in ds.dims if d.startswith('time')]\n",
    "        if not time_dim_candidates:\n",
    "            raise ValueError(\"Aucune dimension time trouvée\")\n",
    "        main_time = [d for d in ds[var].dims if d.startswith('time')][0]\n",
    "        extra_dims = [d for d in time_dim_candidates if d != main_time]\n",
    "        if extra_dims:\n",
    "            ds = ds.drop_dims(extra_dims)\n",
    "        ds = ds.rename({main_time: 'time'})\n",
    "        return ds\n",
    "    \n",
    "    def process_temp(ds):    \n",
    "        for h in height_vars:\n",
    "            if h in ds.coords:\n",
    "                ds = ds.rename({h: 'height_above_ground'})\n",
    "                break\n",
    "\n",
    "        if 'Temperature_height_above_ground' in ds.data_vars:\n",
    "            ds = ds.rename_vars({'Temperature_height_above_ground': 't2m'})\n",
    "\n",
    "        ds = normalize_time(ds, 't2m')        \n",
    "        ds = fix_coords(ds)\n",
    "        \n",
    "        return ds\n",
    "    \n",
    "    def process_precip(ds):\n",
    "        for var in precip_vars:\n",
    "            if var in ds.data_vars:\n",
    "                ds = ds.rename_vars({var: 'tp'})\n",
    "                break        \n",
    "        for tb in time_bounds_vars:\n",
    "            if tb in ds.data_vars:\n",
    "                ds = ds.rename_vars({tb: 'time_bounds'})\n",
    "                break\n",
    "\n",
    "        ds = normalize_time(ds, 'tp')\n",
    "        ds = fix_coords(ds)\n",
    "        \n",
    "        return ds      \n",
    "    \n",
    "    ds_temp = xr.open_mfdataset(f'{place}/{year}/{date}/*.nc', engine='netcdf4', preprocess=process_temp)\n",
    "    ds_precip = xr.open_mfdataset(f'{place}/{year}/{date}/*.nc', engine='netcdf4', preprocess=process_precip)\n",
    "\n",
    "    ds_precip['time'] = ds_temp.time\n",
    "\n",
    "    ds = xr.merge([ds_temp, ds_precip])\n",
    "    t2m_mean = ds.get('t2m').sel(height_above_ground=2).mean(dim='time')\n",
    "\n",
    "    precip = ds.get(['tp', 'time_bounds'])\n",
    "    precip_height = [d for d in precip.dims if d.startswith('height_above_ground')]\n",
    "    if len(precip_height) > 0:\n",
    "        precip = precip.isel({precip_height[0]:2})\n",
    "    precip = precip.sel(time=ds.time.dt.hour % 6 == 0)\n",
    "    bounds = [bound[1] - bound[0] for bound in precip.time_bounds.values]\n",
    "    bounds_6h = all(x == np.timedelta64(21600000000000, 'ns') for x in bounds)\n",
    "\n",
    "    if bounds_6h:\n",
    "        precip_sum = precip.get('tp').sum(dim='time')\n",
    "    else:\n",
    "        precip_sum = None\n",
    "\n",
    "    t2m_df = t2m_mean.to_dataframe().set_index('reftime', append=True)\n",
    "    t2m_df['t2m'] = t2m_df['t2m'] - 273.15\n",
    "    precip_df = precip_sum.to_dataframe().set_index('reftime', append=True)\n",
    "\n",
    "    df = pd.concat([t2m_df, precip_df], axis=1)\n",
    "    if df.empty:\n",
    "        print(f'Empty df for {place}/{year}/{date}')\n",
    "        return 0\n",
    "    df = df[['t2m', 'tp']]\n",
    "    df = df.to_parquet(f'{place}/{year}/{date}_processed.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5c8d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = [i for i in range(2025, 2026)]\n",
    "for year in years:\n",
    "    dates = find_dates(year, start_date=20251106)\n",
    "    for date in dates:\n",
    "        files = find_files(year, date, '00', 168)\n",
    "        for place, coords in places_coordinates.items():\n",
    "            filepath = f'{place}/{year}/{date}'\n",
    "            os.makedirs(filepath, exist_ok=True)\n",
    "            files_tuple = [(year, date, file, coords, filepath) for file in files]\n",
    "            # Télécharger en parallèle\n",
    "            with concurrent.futures.ThreadPoolExecutor(max_workers=14) as executor:\n",
    "                futures = [executor.submit(download_single_file, year_, date_, file_, coords_, filepath_) for year_, date_, file_, coords_, filepath_ in files_tuple]\n",
    "                results = [f.result() for f in futures]\n",
    "            # Vérifier que tout s'est bien passé\n",
    "            if all(results):\n",
    "                for variable in ['Temp', 'Precip']:\n",
    "                    process_files(year, date, place)\n",
    "            else:\n",
    "                failed_files = [file for file, success in zip(files, results) if not success]\n",
    "                print(\"Downloads failed for files:\", failed_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41421e04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2aeacd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5698db3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce87ec73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "effb6f0a",
   "metadata": {},
   "source": [
    "AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b76f5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "GFS_BUCKET_NAME = \"noaa-gfs-bdp-pds\"\n",
    "FORECAST_CYCLE = \"00\"\n",
    "FORECAST_MODEL = \"atmos\"\n",
    "FILE_TYPE = \"pgrb2\"\n",
    "GRID_RESOLUTION = \"0p25\" \n",
    "\n",
    "TEMP_KEYS = {'filter_by_keys': {'typeOfLevel': 'heightAboveGround', 'level': 2, 'cfVarName': 't2m'}, 'indexpath': ''}\n",
    "PRECIP_KEYS = {'filter_by_keys': {'typeOfLevel': 'surface', 'cfVarName': 'tp'}, 'indexpath': ''}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f9ae45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files(s3, date):    \n",
    "    folder = f\"gfs.{date}/{FORECAST_CYCLE}/{FORECAST_MODEL}/\"\n",
    "    try:\n",
    "        response = s3.list_objects_v2(Bucket=GFS_BUCKET_NAME, Prefix=folder, Delimiter='/')\n",
    "        if not 'Contents' in response.keys():\n",
    "            folder = f\"gfs.{date}/{FORECAST_CYCLE}/\"\n",
    "            response = s3.list_objects_v2(Bucket=GFS_BUCKET_NAME, Prefix=folder, Delimiter='/')\n",
    "    except Exception as e:\n",
    "        print(f\"Error accessing S3: {e}\")\n",
    "\n",
    "    pgrb2_files = [obj['Key'] for obj in response['Contents'] if f\"gfs.t{FORECAST_CYCLE}z.{FILE_TYPE}.{GRID_RESOLUTION}.f\" in obj['Key']]\n",
    "    file_res = [file for file in pgrb2_files if file[-4:] != \".idx\"]\n",
    "    file_horizon = [\n",
    "        file for file in file_res\n",
    "        if 0 < (h := int(file.split('.')[5][-3:])) <= 168\n",
    "        and (\n",
    "            (h <= 84 and h % 3 == 0) or\n",
    "            (h > 84 and h % 6 == 0)\n",
    "        )\n",
    "    ]\n",
    "                    \n",
    "    return file_horizon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9138012c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(s3, file, filepath):\n",
    "    try:\n",
    "        filename = file.split('/')[-1]\n",
    "        os.makedirs(filepath, exist_ok=True)\n",
    "        s3.download_file(GFS_BUCKET_NAME, file, f'{filepath}/{filename}')\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading {file}: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eab0a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_files(year, date, places):\n",
    "    files = sorted(glob.glob(f'{year}/{date}/*'))  \n",
    "\n",
    "    acc = None\n",
    "    n = 0\n",
    "\n",
    "    for f in files:\n",
    "        #ds = xr.open_dataset(f, engine='cfgrib', backend_kwargs=kwargs, decode_timedelta=True)\n",
    "        ds_t2m = xr.open_dataset(f, engine=\"cfgrib\", backend_kwargs=TEMP_KEYS, decode_timedelta=True)\n",
    "        ds_tp = xr.open_dataset(f, engine=\"cfgrib\", backend_kwargs=PRECIP_KEYS, decode_timedelta=True)\n",
    "        ds = xr.merge([ds_t2m, ds_tp])\n",
    "        \n",
    "        if ds.longitude.max() > 180:\n",
    "            ds = ds.assign_coords(longitude=((ds.longitude + 180) % 360) - 180)\n",
    "            ds = ds.sortby(\"longitude\")\n",
    "\n",
    "        if acc is None:\n",
    "            acc = ds.copy(deep=False)\n",
    "        else:\n",
    "            acc += ds\n",
    "\n",
    "        n += 1\n",
    "\n",
    "    t2m_mean = acc['t2m'] / n\n",
    "    t2m_mean -= 273.15\n",
    "    precip_sum = acc['tp']\n",
    "\n",
    "    for place, coords in places.items():\n",
    "        sl = dict(\n",
    "            latitude=slice(coords['lat_max'], coords['lat_min']),\n",
    "            longitude=slice(coords['lon_min'], coords['lon_max'])\n",
    "        )\n",
    "        t2m_sliced = t2m_mean.sel(**sl)\n",
    "        precip_sliced = precip_sum.sel(**sl)\n",
    "    \n",
    "        df_t2m = t2m_sliced.to_dataframe().reset_index()\n",
    "        df_t2m = df_t2m[['time', 'latitude', 'longitude', 't2m']]\n",
    "\n",
    "        df_precip = precip_sliced.to_dataframe().reset_index()\n",
    "        df_precip = df_precip[['time', 'latitude', 'longitude', 'tp']]\n",
    "        \n",
    "        if df_t2m.empty or df_precip.empty:\n",
    "            raise ValueError(f\"{variable}, {date}, one of the 2 df is empty\")\n",
    "\n",
    "        os.makedirs(f\"Temp/{place}/{year}/{date}\", exist_ok=True)\n",
    "        os.makedirs(f\"Precip/{place}/{year}/{date}\", exist_ok=True)\n",
    "        df_t2m.to_parquet(f\"Temp/{place}/{year}/{date}_processed.parquet\", compression=\"snappy\")\n",
    "        df_precip.to_parquet(f\"Precip/{place}/{year}/{date}_processed.parquet\", compression=\"snappy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1b8f3e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dates(s3):\n",
    "    paginator = s3.get_paginator(\"list_objects_v2\")\n",
    "    pattern = re.compile(r\"gfs\\.\\d{8}/$\")\n",
    "    gfs_dates = []\n",
    "\n",
    "    for page in paginator.paginate(Bucket=GFS_BUCKET_NAME, Delimiter=\"/\"):\n",
    "        for cp in page.get(\"CommonPrefixes\", []):\n",
    "            prefix = cp[\"Prefix\"]\n",
    "            if pattern.match(prefix):\n",
    "                file_name = prefix.rstrip(\"/\")\n",
    "                gfs_dates.append(file_name.split('.')[1])\n",
    "    return gfs_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a36102f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "s3 = boto3.client('s3', config=Config(signature_version=UNSIGNED))\n",
    "dates = get_dates(s3)\n",
    "for date in dates:\n",
    "    files = get_files(s3, date)\n",
    "    year = files[0].split('/')[0][4:8]\n",
    "    files_tuple = [(file, f'{year}/{date}') for file in files]\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=8) as executor:\n",
    "        futures = [executor.submit(download_file, s3, file, path) for file, path in files_tuple]\n",
    "        results = [f.result() for f in futures]\n",
    "    if all(results):\n",
    "        print(\"ok\")\n",
    "        process_files(year, date, places_coordinates)\n",
    "        shutil.rmtree(f'{year}/{date}')\n",
    "    else:\n",
    "        failed_files = [file for file, success in zip(files, results) if not success]\n",
    "        print(\"Downloads failed for files:\", failed_files)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f27f32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet('Precip/Europe/2020/20200101_processed.parquet')\n",
    "\n",
    "fig = px.scatter_geo(\n",
    "    df,\n",
    "    lat=\"latitude\",\n",
    "    lon=\"longitude\",\n",
    "    color=\"Total_precipitation_surface_Mixed_intervals_Accumulation\",\n",
    "    labels={\"Total_precipitation_surface_Mixed_intervals_Accumulation\": \"Total precip\"},\n",
    "    color_continuous_scale=px.colors.sequential.Viridis\n",
    ")\n",
    "fig.update_geos(fitbounds=\"locations\")\n",
    "fig.update_layout(title=\"Total precip\", title_x=0.5)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26980582",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
