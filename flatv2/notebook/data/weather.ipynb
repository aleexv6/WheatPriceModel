{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf75f2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore import UNSIGNED\n",
    "from botocore.config import Config\n",
    "import xarray as xr\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "import concurrent.futures\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63b7dad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "places_coordinates = {\n",
    "    \"Europe\": {'lat_min': 35.25, 'lat_max': 71.75, 'lon_min': -10.25, 'lon_max': 31},\n",
    "    \"Canada\": {'lat_min': 44.0, 'lat_max': 70.0, 'lon_min': -141.5, 'lon_max': -51.5}, \n",
    "    \"US\": {'lat_min': 24, 'lat_max': 49, 'lon_min': -125.5, 'lon_max': -65.5},\n",
    "    \"BlackSea\": {'lat_min': 41.25, 'lat_max': 57.0, 'lon_min':22.75, 'lon_max': 61.5},\n",
    "    \"Australia\": {'lat_min': -39, 'lat_max': -10.5, 'lon_min': 112.75, 'lon_max': 153.25},\n",
    "    \"Argentina\": {'lat_min': -56.25, 'lat_max': -21.75, 'lon_min': -75.5, 'lon_max': -51.75}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a704f6a6",
   "metadata": {},
   "source": [
    "Before 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aece7d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_dates(year, max_retries=5, start_date=False):\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            r = requests.get(f'https://tds.gdex.ucar.edu/thredds/catalog/files/g/d084001/{year}/catalog.html', timeout=10)\n",
    "            break\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            retries += 1\n",
    "            time.sleep(5)\n",
    "            \n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "    dates = [f.get_text() for f in soup.find_all('code') if re.fullmatch(r\"\\d{8}\", f.get_text())]\n",
    "    if start_date:\n",
    "        return [d for d in dates if datetime.strptime(d, '%Y%m%d') >= datetime.strptime(str(start_date), '%Y%m%d')]\n",
    "    else:\n",
    "        return dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b9a380e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_files(year, date, run, variable, forecast_horizon, max_retries=5):\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            r = requests.get(f'https://tds.gdex.ucar.edu/thredds/catalog/files/g/d084001/{year}/{date}/catalog.html')\n",
    "            break\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            retries += 1\n",
    "            time.sleep(5)\n",
    "            \n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "    files =  [f.get_text() for f in soup.find_all('code') if f.get_text()[-6:] == '.grib2']\n",
    "    current_run = [f for f in files if f.split('.')[2][-2:] == run  # check if run is 00\n",
    "                   and int(f.split('.f')[1][:3]) > 0 #remove the first data, where data is initial values of the model and not forecast\n",
    "                   and int(f.split('.f')[1][:3]) <= forecast_horizon] #where files are max forecast horizon\n",
    "    precip_run = [f for f in current_run if int(f.split('.f')[1][:3]) > 0 #remove the first data, where data is initial values of the model and not forecast\n",
    "                  and int(f.split('.f')[1][:3]) % 6 == 0 #every 6 hours\n",
    "                  and int(f.split('.f')[1][:3]) <= forecast_horizon] #where files are max forecast horizon\n",
    "\n",
    "    match variable:\n",
    "        case 'Temp':\n",
    "            return current_run\n",
    "        case 'Precip':\n",
    "            return precip_run\n",
    "        case _:\n",
    "            raise ValueError('Variable pas connue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f1ae4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_single_file(year, date, file, coords, variable, filepath, max_retries=5):\n",
    "    forecast_hour = int(file.split('.f')[1][:3])\n",
    "    forecast_hour_timedelta = timedelta(hours=forecast_hour)\n",
    "    ref_date =  datetime.strptime(file.split('.')[2], \"%Y%m%d%H\")\n",
    "    final_dt = ref_date + forecast_hour_timedelta\n",
    "    iso_date = final_dt.strftime(\"%Y-%m-%dT%H:%M:%SZ\")        \n",
    "            \n",
    "    match variable:\n",
    "        case 'Temp':\n",
    "            var = 'var=Temperature_height_above_ground'\n",
    "        case 'Precip':\n",
    "            if ref_date < datetime(2019, 6, 13):\n",
    "                var = 'var=Total_precipitation_surface_6_Hour_Accumulation'\n",
    "            elif forecast_hour == 6: \n",
    "                var = 'var=Total_precipitation_surface_6_Hour_Accumulation'\n",
    "            else:\n",
    "                var = 'var=Total_precipitation_surface_Mixed_intervals_Accumulation'\n",
    "\n",
    "    endpoint = \"https://tds.gdex.ucar.edu/thredds/ncss/grid/files/g/d084001/\"\n",
    "    url_params = f\"{year}/{date}/{file}?{var}&\" \\\n",
    "                        f\"north={coords['lat_max']}&west={coords['lon_min']}&east={coords['lon_max']}&south={coords['lat_min']}&\" \\\n",
    "                        f\"horizStride=1&time_start={iso_date}&time_end={iso_date}&&&accept=netcdf3\"\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            r = requests.get(endpoint+url_params, timeout=10)\n",
    "            r.raise_for_status()\n",
    "            with open(f'{filepath}/gfs.0p25.{iso_date.replace(\":\", \"-\")}.grib2.nc', \"wb\") as f:\n",
    "                f.write(r.content)\n",
    "            break\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            retries += 1\n",
    "            time.sleep(1)\n",
    "    else:\n",
    "        print(\"Max retries reached. Download failed for \", file)\n",
    "\n",
    "def download_wrapper(file):\n",
    "    \"\"\"Wrapper pour gérer les retries et exceptions par fichier.\"\"\"\n",
    "    try:\n",
    "        download_single_file(year, date, file, coords, variable, f\"{variable}/{place}/{year}/{date}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Download failed for {file}: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b2e2dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_files(year, date, place, variable):\n",
    "    def fix_coords(ds):\n",
    "        # Harmoniser la coordonnée temporelle\n",
    "        if \"time1\" in ds.coords:      # si le fichier contient \"time1\"\n",
    "            ds = ds.rename({\"time1\": \"time\"})\n",
    "        if \"time2\" in ds.coords:\n",
    "            ds = ds.rename({\"time2\": \"time\"})\n",
    "        if \"height_above_ground1\" in ds.coords:\n",
    "            ds = ds.rename({\"height_above_ground1\": \"height_above_ground\"})\n",
    "        if \"height_above_ground2\" in ds.coords:\n",
    "            ds = ds.rename({\"height_above_ground2\": \"height_above_ground\"})\n",
    "        if \"height_above_ground3\" in ds.coords:\n",
    "            ds = ds.rename({\"height_above_ground3\": \"height_above_ground\"})\n",
    "        # Same for precip variable name\n",
    "        if \"Total_precipitation_surface_6_Hour_Accumulation\" in ds.data_vars:\n",
    "            ds = ds.rename_vars({\"Total_precipitation_surface_6_Hour_Accumulation\": \"Total_precipitation_surface_Mixed_intervals_Accumulation\"})\n",
    "        return ds\n",
    "    \n",
    "    ds = xr.open_mfdataset(f'{variable}/{place}/{year}/{date}/*.nc', engine='netcdf4', preprocess=fix_coords, decode_timedelta=True)\n",
    "\n",
    "    if ds.longitude.max() > 180:\n",
    "        ds = ds.assign_coords(longitude=((ds.longitude + 180) % 360) - 180)\n",
    "        ds = ds.sortby(\"longitude\")\n",
    "\n",
    "    match variable:\n",
    "        case 'Temp':\n",
    "            ds_2m = ds.sel(height_above_ground=2)\n",
    "            t2m = ds_2m['Temperature_height_above_ground'].mean(dim='time')\n",
    "            df_t2m = t2m.to_dataframe().reset_index()\n",
    "            df_t2m['Temperature_height_above_ground'] = df_t2m['Temperature_height_above_ground'] - 273.15\n",
    "            df_t2m = df_t2m[['reftime', 'Temperature_height_above_ground', 'latitude', 'longitude']]\n",
    "            if not df_t2m.empty:\n",
    "                df_t2m.to_parquet(f\"{variable}/{place}/{year}/{date}_processed.parquet\", compression='snappy')\n",
    "                #shutil.rmtree(f\"{variable}/{place}/{year}/{date}\")\n",
    "            else:\n",
    "                raise ValueError(f\"{variable}, {date}, df is empty\")\n",
    "        case 'Precip':\n",
    "            precip = ds['Total_precipitation_surface_Mixed_intervals_Accumulation'].sum(dim='time')\n",
    "            df_precip = precip.to_dataframe().reset_index()\n",
    "            df_precip = df_precip[['reftime', 'Total_precipitation_surface_Mixed_intervals_Accumulation', 'latitude', 'longitude']]\n",
    "            if not df_precip.empty:\n",
    "                df_precip.to_parquet(f\"{variable}/{place}/{year}/{date}_processed.parquet\", compression='snappy')\n",
    "                #shutil.rmtree(f\"{variable}/{place}/{year}/{date}\")\n",
    "            else:\n",
    "                raise ValueError(f\"{variable}, {date}, df is empty\")\n",
    "        case _:\n",
    "            raise ValueError(\"Variable needs to be 'Temp' or 'Precip'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5c8d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = [i for i in range(2015, 2022)]\n",
    "for variable in ['Temp', 'Precip']:   \n",
    "    if variable == 'Temp': #Don't do one of them\n",
    "        continue\n",
    "    os.makedirs(f\"{variable}\", exist_ok=True)\n",
    "    for place, coords in places_coordinates.items():\n",
    "        if place == \"Europe\" or place == \"Canada\": # Move to next place if needed\n",
    "            continue\n",
    "        os.makedirs(f\"{variable}/{place}\", exist_ok=True)\n",
    "        for year in years:\n",
    "            os.makedirs(f\"{variable}/{place}/{year}\", exist_ok=True)\n",
    "            dates = find_dates(year)\n",
    "            for date in dates:\n",
    "                os.makedirs(f\"{variable}/{place}/{year}/{date}\", exist_ok=True)\n",
    "                files = find_files(year, date, '00', variable, 168)\n",
    "                # Télécharger en parallèle\n",
    "                with concurrent.futures.ThreadPoolExecutor(max_workers=14) as executor:  # Ajuste le nombre de threads\n",
    "                    results = list(executor.map(download_wrapper, files))\n",
    "\n",
    "                # Vérifier que tout s'est bien passé\n",
    "                if all(results):\n",
    "                    process_files(year, date, place, variable)\n",
    "                else:\n",
    "                    failed_files = [file for file, success in zip(files, results) if not success]\n",
    "                    print(\"Downloads failed for files:\", failed_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a706265c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet('Temp/Argentina/2015/20150115_processed.parquet')\n",
    "\n",
    "fig = px.scatter_geo(\n",
    "    df,\n",
    "    lat=\"latitude\",\n",
    "    lon=\"longitude\",\n",
    "    color=\"Temperature_height_above_ground\",\n",
    "    labels={\"Temperature_height_above_ground\": \"Temp\"},\n",
    "    color_continuous_scale=px.colors.sequential.Viridis\n",
    ")\n",
    "fig.update_geos(fitbounds=\"locations\")\n",
    "fig.update_layout(title=\"Temperature_height_above_ground\", title_x=0.5)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effb6f0a",
   "metadata": {},
   "source": [
    "After 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b76f5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "GFS_BUCKET_NAME = \"noaa-gfs-bdp-pds\"\n",
    "FORECAST_CYCLE = \"00\"\n",
    "FORECAST_MODEL = \"atmos\"\n",
    "FILE_TYPE = \"pgrb2\"\n",
    "GRID_RESOLUTION = \"0p25\" \n",
    "\n",
    "TEMP_KEYS = {'filter_by_keys': {'typeOfLevel': 'heightAboveGround', 'level': 2, 'cfVarName': 't2m'}, 'indexpath': ''}\n",
    "PRECIP_KEYS = {'filter_by_keys': {'typeOfLevel': 'surface', 'cfVarName': 'tp'}, 'indexpath': ''}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f9ae45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files(s3, date):    \n",
    "    folder = f\"gfs.{date}/{FORECAST_CYCLE}/{FORECAST_MODEL}/\"\n",
    "    try:\n",
    "        response = s3.list_objects_v2(Bucket=GFS_BUCKET_NAME, Prefix=folder, Delimiter='/')\n",
    "        if not 'Contents' in response.keys():\n",
    "            folder = f\"gfs.{date}/{FORECAST_CYCLE}/\"\n",
    "            response = s3.list_objects_v2(Bucket=GFS_BUCKET_NAME, Prefix=folder, Delimiter='/')\n",
    "    except Exception as e:\n",
    "        print(f\"Error accessing S3: {e}\")\n",
    "\n",
    "    pgrb2_files = [obj['Key'] for obj in response['Contents'] if f\"gfs.t{FORECAST_CYCLE}z.{FILE_TYPE}.{GRID_RESOLUTION}.f\" in obj['Key']]\n",
    "    file_res = [file for file in pgrb2_files if file[-4:] != \".idx\"]\n",
    "    file_horizon = [file for file in file_res if int(file.split('.')[5][-3:]) <= 168\n",
    "                    and int(file.split('.')[5][-3:]) > 0\n",
    "                    and int(file.split('.')[5][-3:]) % 3 == 0]\n",
    "                    \n",
    "    return file_horizon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9138012c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_files(s3, files, filepath):\n",
    "    for file in files:\n",
    "        try:\n",
    "            filename = file.split('/')[-1]\n",
    "            os.makedirs(filepath, exist_ok=True)\n",
    "            s3.download_file(GFS_BUCKET_NAME, file, f'{filepath}/{filename}')\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading {file}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6eab0a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_files(year, date, places):\n",
    "    files = sorted(glob.glob(f'{year}/{date}/*'))  \n",
    "\n",
    "    acc = None\n",
    "    n = 0\n",
    "\n",
    "    for f in files:\n",
    "        #ds = xr.open_dataset(f, engine='cfgrib', backend_kwargs=kwargs, decode_timedelta=True)\n",
    "        ds_t2m = xr.open_dataset(f, engine=\"cfgrib\", backend_kwargs=TEMP_KEYS, decode_timedelta=True)\n",
    "        ds_tp = xr.open_dataset(f, engine=\"cfgrib\", backend_kwargs=PRECIP_KEYS, decode_timedelta=True)\n",
    "        ds = xr.merge([ds_t2m, ds_tp])\n",
    "        \n",
    "        if ds.longitude.max() > 180:\n",
    "            ds = ds.assign_coords(longitude=((ds.longitude + 180) % 360) - 180)\n",
    "            ds = ds.sortby(\"longitude\")\n",
    "\n",
    "        if acc is None:\n",
    "            acc = ds.copy(deep=False)\n",
    "        else:\n",
    "            acc += ds\n",
    "\n",
    "        n += 1\n",
    "\n",
    "    t2m_mean = acc['t2m'] / n\n",
    "    t2m_mean -= 273.15\n",
    "    precip_sum = acc['tp']\n",
    "\n",
    "    for place, coords in places.items():\n",
    "        sl = dict(\n",
    "            latitude=slice(coords['lat_max'], coords['lat_min']),\n",
    "            longitude=slice(coords['lon_min'], coords['lon_max'])\n",
    "        )\n",
    "        t2m_sliced = t2m_mean.sel(**sl)\n",
    "        precip_sliced = precip_sum.sel(**sl)\n",
    "    \n",
    "        df_t2m = t2m_sliced.to_dataframe().reset_index()\n",
    "        df_t2m = df_t2m[['time', 'latitude', 'longitude', 't2m']]\n",
    "\n",
    "        df_precip = precip_sliced.to_dataframe().reset_index()\n",
    "        df_precip = df_precip[['time', 'latitude', 'longitude', 'tp']]\n",
    "        \n",
    "        if df_t2m.empty or df_precip.empty:\n",
    "            raise ValueError(f\"{variable}, {date}, one of the 2 df is empty\")\n",
    "\n",
    "        df_t2m.to_parquet(f\"Temp/{place}/{year}/{date}_processed.parquet\", compression=\"snappy\")\n",
    "        df_precip.to_parquet(f\"Precip/{place}/{year}/{date}_processed.parquet\", compression=\"snappy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b8f3e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dates(s3):\n",
    "    paginator = s3.get_paginator(\"list_objects_v2\")\n",
    "    pattern = re.compile(r\"gfs\\.\\d{8}/$\")\n",
    "    gfs_dates = []\n",
    "\n",
    "    for page in paginator.paginate(Bucket=GFS_BUCKET_NAME, Delimiter=\"/\"):\n",
    "        for cp in page.get(\"CommonPrefixes\", []):\n",
    "            prefix = cp[\"Prefix\"]\n",
    "            if pattern.match(prefix):\n",
    "                file_name = prefix.rstrip(\"/\")\n",
    "                gfs_dates.append(file_name.split('.')[1])\n",
    "    return gfs_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a36102f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexl\\miniconda3\\envs\\conda-env\\Lib\\site-packages\\xarray\\backends\\plugins.py:110: RuntimeWarning: Engine 'rasterio' loading failed:\n",
      "DLL load failed while importing _io: Le module spécifié est introuvable.\n",
      "  external_backend_entrypoints = backends_dict_from_pkg(entrypoints_unique)\n"
     ]
    }
   ],
   "source": [
    "s3 = boto3.client('s3', config=Config(signature_version=UNSIGNED))\n",
    "dates = get_dates(s3)\n",
    "for date in dates:\n",
    "    files = get_files(s3, date)\n",
    "    year = files[0].split('/')[0][4:8]\n",
    "    download_files(s3, files, f'{year}/{date}') \n",
    "    process_files(year, date, places_coordinates)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab56e9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parralel download of file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2464cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
